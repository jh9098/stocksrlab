import os
import json
import shutil
from crawler import crawl_naver_price
from datetime import datetime

CRAWLED_DIR = "./frontend/public/data/crawled"
PUBLIC_DIR = "./frontend/public/data/crawled"
INDEX_PATH = os.path.join(CRAWLED_DIR, "index.json")
PUBLIC_INDEX_PATH = os.path.join(PUBLIC_DIR, "index.json")

def load_existing_index():
    if not os.path.exists(INDEX_PATH):
        return {}
    with open(INDEX_PATH, "r", encoding="utf-8") as f:
        return json.load(f)

def update_one_stock(code, existing_prices):
    full_code = "A" + code
    latest = crawl_naver_price(full_code, max_pages=5)

    last_date = existing_prices[-1]["date"] if existing_prices else "1900-01-01"
    new_data = [p for p in latest if p["date"] > last_date]

    if not new_data:
        print(f"â© {code}: ìƒˆ ë°ì´í„° ì—†ìŒ (ë§ˆì§€ë§‰ ë‚ ì§œ {last_date})")
        return existing_prices

    merged = existing_prices + new_data
    merged = sorted(merged, key=lambda x: x["date"])

    print(f"âœ… {code} ì—…ë°ì´íŠ¸ ì™„ë£Œ (+{len(new_data)}ê±´)")
    return merged

def update_all_crawled():
    index_data = load_existing_index()
    codes = sorted(index_data.keys())

    print(f"ğŸ” ì´ {len(codes)}ê°œ ì¢…ëª© ëˆ„ì  ì—…ë°ì´íŠ¸ ì‹œì‘")

    for i, code in enumerate(codes):
        print(f"[{i+1}/{len(codes)}] {code}")
        try:
            existing = index_data.get(code, [])
            updated = update_one_stock(code, existing)
            index_data[code] = updated
        except Exception as e:
            print(f"âŒ {code} ì—ëŸ¬ ë°œìƒ: {e}")

    # ì €ì¥: srcìš©
    with open(INDEX_PATH, "w", encoding="utf-8") as f:
        json.dump(index_data, f, ensure_ascii=False, separators=(",", ":"))

    # ì €ì¥: Netlifyìš© public ë””ë ‰í† ë¦¬
    os.makedirs(PUBLIC_DIR, exist_ok=True)
    shutil.copy(INDEX_PATH, PUBLIC_INDEX_PATH)

    print(f"\nâœ… ì „ì²´ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    print(f"ğŸ“¦ ì €ì¥ ìœ„ì¹˜ (src): {INDEX_PATH}")
    print(f"ğŸŒ ì €ì¥ ìœ„ì¹˜ (public): {PUBLIC_INDEX_PATH}")

if __name__ == "__main__":
    update_all_crawled()
